{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys \n",
    "sys.path.insert(0, str(Path(os.getcwd()).parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from lib.models.configuration_mono3dvg_v2 import Mono3DVGv2Config\n",
    "from lib.models.mono3dvg_v2 import Mono3DVGv2ForSingleObjectDetection\n",
    "from lib.models.image_processsing_mono3dvg import Mono3DVGImageProcessor\n",
    "from utils.parser import load_yaml_config, dict_to_namespace, namespace_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dict_to_namespace(load_yaml_config({}, Path(\"../../configs/vl-swin-dab.yaml\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.text_encoder_type = \"../../pretrained-models/roberta-base\"\n",
    "cfg.model.pretrained_backbone_path = '../../pretrained-models/swin_large_patch4_window7_224/model.safetensors'\n",
    "config = Mono3DVGv2Config(**namespace_to_dict(cfg.model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mono3DVGv2Config {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"relu\",\n",
       "  \"angle_loss_coefficient\": 1,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auxiliary_loss\": false,\n",
       "  \"backbone\": \"swin_large_patch4_window7_224\",\n",
       "  \"backbone_config\": null,\n",
       "  \"bbox_cost\": 5,\n",
       "  \"bbox_loss_coefficient\": 5,\n",
       "  \"center3d_cost\": 10,\n",
       "  \"center3d_loss_coefficient\": 10,\n",
       "  \"class_cost\": 2,\n",
       "  \"cls_loss_coefficient\": 2,\n",
       "  \"d_model\": 256,\n",
       "  \"decoder_attention_heads\": 8,\n",
       "  \"decoder_depth_residual\": false,\n",
       "  \"decoder_ffn_dim\": 256,\n",
       "  \"decoder_layers\": 1,\n",
       "  \"decoder_n_points\": 4,\n",
       "  \"decoder_self_attn\": false,\n",
       "  \"decoder_text_residual\": false,\n",
       "  \"depth_loss_coefficient\": 1,\n",
       "  \"depth_map_loss_coefficient\": 1,\n",
       "  \"depth_max\": 60.0,\n",
       "  \"depth_min\": 0.001,\n",
       "  \"dim_loss_coefficient\": 1,\n",
       "  \"disable_custom_kernels\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 8,\n",
       "  \"encoder_ffn_dim\": 256,\n",
       "  \"encoder_layers\": 3,\n",
       "  \"encoder_n_points\": 4,\n",
       "  \"focal_alpha\": 0.25,\n",
       "  \"giou_cost\": 2,\n",
       "  \"giou_loss_coefficient\": 2,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\"\n",
       "  },\n",
       "  \"init_box\": false,\n",
       "  \"init_std\": 0.02,\n",
       "  \"init_xavier_std\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8\n",
       "  },\n",
       "  \"model_type\": \"mono3dvgv2\",\n",
       "  \"num_channels\": 3,\n",
       "  \"num_depth_bins\": 80,\n",
       "  \"num_feature_levels\": 4,\n",
       "  \"num_queries\": 1,\n",
       "  \"num_text_output_layers\": 1,\n",
       "  \"position_embedding_type\": \"sine\",\n",
       "  \"pretrained_backbone_path\": \"../../pretrained-models/swin_large_patch4_window7_224/model.safetensors\",\n",
       "  \"text_encoder_type\": \"../../pretrained-models/roberta-base\",\n",
       "  \"transformers_version\": \"4.44.0\",\n",
       "  \"two_stage\": false,\n",
       "  \"use_dab\": true,\n",
       "  \"use_pretrained_backbone\": true,\n",
       "  \"use_text_guided_adapter\": true,\n",
       "  \"use_timm_backbone\": true,\n",
       "  \"vl_encoder_type\": \"fusion\",\n",
       "  \"with_box_refine\": true\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\Mono3DVGv2\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "e:\\mycode\\pycode\\3DObject\\Mono3DVGv2\\lib\\models\\mono3dvg_v2.py:1332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained_model_name_or_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# model = Mono3DVGv2ForSingleObjectDetection(config)\n",
    "model, loading_info = Mono3DVGv2ForSingleObjectDetection._load_mono3dvg_pretrained_model(\"../../pretrained-models/mono3dvg-vl-swin-dab/checkpoint_best.pth\", config, output_loading_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_keys': [],\n",
       " 'unexpected_keys': ['model.reference_points.weight',\n",
       "  'model.reference_points.bias']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.level_embed',\n",
       " 'model.input_proj.0.0.weight',\n",
       " 'model.input_proj.0.0.bias',\n",
       " 'model.input_proj.0.1.weight',\n",
       " 'model.input_proj.0.1.bias',\n",
       " 'model.input_proj.1.0.weight',\n",
       " 'model.input_proj.1.0.bias',\n",
       " 'model.input_proj.1.1.weight',\n",
       " 'model.input_proj.1.1.bias',\n",
       " 'model.input_proj.2.0.weight',\n",
       " 'model.input_proj.2.0.bias',\n",
       " 'model.input_proj.2.1.weight',\n",
       " 'model.input_proj.2.1.bias',\n",
       " 'model.input_proj.3.0.weight',\n",
       " 'model.input_proj.3.0.bias',\n",
       " 'model.input_proj.3.1.weight',\n",
       " 'model.input_proj.3.1.bias',\n",
       " 'model.language_proj.0.0.weight',\n",
       " 'model.language_proj.0.0.bias',\n",
       " 'model.language_proj.0.1.weight',\n",
       " 'model.language_proj.0.1.bias',\n",
       " 'model.encoder.layers.0.self_attn.sampling_offsets.weight',\n",
       " 'model.encoder.layers.0.self_attn.sampling_offsets.bias',\n",
       " 'model.encoder.layers.0.self_attn.attention_weights.weight',\n",
       " 'model.encoder.layers.0.self_attn.attention_weights.bias',\n",
       " 'model.encoder.layers.0.self_attn.value_proj.weight',\n",
       " 'model.encoder.layers.0.self_attn.value_proj.bias',\n",
       " 'model.encoder.layers.0.self_attn.output_proj.weight',\n",
       " 'model.encoder.layers.0.self_attn.output_proj.bias',\n",
       " 'model.encoder.layers.0.self_attn_layer_norm.weight',\n",
       " 'model.encoder.layers.0.self_attn_layer_norm.bias',\n",
       " 'model.encoder.layers.0.cross_attn_text.in_proj_weight',\n",
       " 'model.encoder.layers.0.cross_attn_text.in_proj_bias',\n",
       " 'model.encoder.layers.0.cross_attn_text.out_proj.weight',\n",
       " 'model.encoder.layers.0.cross_attn_text.out_proj.bias',\n",
       " 'model.encoder.layers.0.cross_attn_text_layer_norm.weight',\n",
       " 'model.encoder.layers.0.cross_attn_text_layer_norm.bias',\n",
       " 'model.encoder.layers.0.fc1.weight',\n",
       " 'model.encoder.layers.0.fc1.bias',\n",
       " 'model.encoder.layers.0.fc2.weight',\n",
       " 'model.encoder.layers.0.fc2.bias',\n",
       " 'model.encoder.layers.0.final_layer_norm.weight',\n",
       " 'model.encoder.layers.0.final_layer_norm.bias',\n",
       " 'model.encoder.layers.1.self_attn.sampling_offsets.weight',\n",
       " 'model.encoder.layers.1.self_attn.sampling_offsets.bias',\n",
       " 'model.encoder.layers.1.self_attn.attention_weights.weight',\n",
       " 'model.encoder.layers.1.self_attn.attention_weights.bias',\n",
       " 'model.encoder.layers.1.self_attn.value_proj.weight',\n",
       " 'model.encoder.layers.1.self_attn.value_proj.bias',\n",
       " 'model.encoder.layers.1.self_attn.output_proj.weight',\n",
       " 'model.encoder.layers.1.self_attn.output_proj.bias',\n",
       " 'model.encoder.layers.1.self_attn_layer_norm.weight',\n",
       " 'model.encoder.layers.1.self_attn_layer_norm.bias',\n",
       " 'model.encoder.layers.1.cross_attn_text.in_proj_weight',\n",
       " 'model.encoder.layers.1.cross_attn_text.in_proj_bias',\n",
       " 'model.encoder.layers.1.cross_attn_text.out_proj.weight',\n",
       " 'model.encoder.layers.1.cross_attn_text.out_proj.bias',\n",
       " 'model.encoder.layers.1.cross_attn_text_layer_norm.weight',\n",
       " 'model.encoder.layers.1.cross_attn_text_layer_norm.bias',\n",
       " 'model.encoder.layers.1.fc1.weight',\n",
       " 'model.encoder.layers.1.fc1.bias',\n",
       " 'model.encoder.layers.1.fc2.weight',\n",
       " 'model.encoder.layers.1.fc2.bias',\n",
       " 'model.encoder.layers.1.final_layer_norm.weight',\n",
       " 'model.encoder.layers.1.final_layer_norm.bias',\n",
       " 'model.encoder.layers.2.self_attn.sampling_offsets.weight',\n",
       " 'model.encoder.layers.2.self_attn.sampling_offsets.bias',\n",
       " 'model.encoder.layers.2.self_attn.attention_weights.weight',\n",
       " 'model.encoder.layers.2.self_attn.attention_weights.bias',\n",
       " 'model.encoder.layers.2.self_attn.value_proj.weight',\n",
       " 'model.encoder.layers.2.self_attn.value_proj.bias',\n",
       " 'model.encoder.layers.2.self_attn.output_proj.weight',\n",
       " 'model.encoder.layers.2.self_attn.output_proj.bias',\n",
       " 'model.encoder.layers.2.self_attn_layer_norm.weight',\n",
       " 'model.encoder.layers.2.self_attn_layer_norm.bias',\n",
       " 'model.encoder.layers.2.cross_attn_text.in_proj_weight',\n",
       " 'model.encoder.layers.2.cross_attn_text.in_proj_bias',\n",
       " 'model.encoder.layers.2.cross_attn_text.out_proj.weight',\n",
       " 'model.encoder.layers.2.cross_attn_text.out_proj.bias',\n",
       " 'model.encoder.layers.2.cross_attn_text_layer_norm.weight',\n",
       " 'model.encoder.layers.2.cross_attn_text_layer_norm.bias',\n",
       " 'model.encoder.layers.2.fc1.weight',\n",
       " 'model.encoder.layers.2.fc1.bias',\n",
       " 'model.encoder.layers.2.fc2.weight',\n",
       " 'model.encoder.layers.2.fc2.bias',\n",
       " 'model.encoder.layers.2.final_layer_norm.weight',\n",
       " 'model.encoder.layers.2.final_layer_norm.bias',\n",
       " 'model.decoder.layers.0.cross_attn_text.in_proj_weight',\n",
       " 'model.decoder.layers.0.cross_attn_text.in_proj_bias',\n",
       " 'model.decoder.layers.0.cross_attn_text.out_proj.weight',\n",
       " 'model.decoder.layers.0.cross_attn_text.out_proj.bias',\n",
       " 'model.decoder.layers.0.cross_attn_text_layer_norm.weight',\n",
       " 'model.decoder.layers.0.cross_attn_text_layer_norm.bias',\n",
       " 'model.decoder.layers.0.cross_attn_depth.in_proj_weight',\n",
       " 'model.decoder.layers.0.cross_attn_depth.in_proj_bias',\n",
       " 'model.decoder.layers.0.cross_attn_depth.out_proj.weight',\n",
       " 'model.decoder.layers.0.cross_attn_depth.out_proj.bias',\n",
       " 'model.decoder.layers.0.cross_attn_depth_layer_norm.weight',\n",
       " 'model.decoder.layers.0.cross_attn_depth_layer_norm.bias',\n",
       " 'model.decoder.layers.0.encoder_attn.sampling_offsets.weight',\n",
       " 'model.decoder.layers.0.encoder_attn.sampling_offsets.bias',\n",
       " 'model.decoder.layers.0.encoder_attn.attention_weights.weight',\n",
       " 'model.decoder.layers.0.encoder_attn.attention_weights.bias',\n",
       " 'model.decoder.layers.0.encoder_attn.value_proj.weight',\n",
       " 'model.decoder.layers.0.encoder_attn.value_proj.bias',\n",
       " 'model.decoder.layers.0.encoder_attn.output_proj.weight',\n",
       " 'model.decoder.layers.0.encoder_attn.output_proj.bias',\n",
       " 'model.decoder.layers.0.encoder_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.0.encoder_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.0.fc1.weight',\n",
       " 'model.decoder.layers.0.fc1.bias',\n",
       " 'model.decoder.layers.0.fc2.weight',\n",
       " 'model.decoder.layers.0.fc2.bias',\n",
       " 'model.decoder.layers.0.final_layer_norm.weight',\n",
       " 'model.decoder.layers.0.final_layer_norm.bias',\n",
       " 'model.decoder.query_scale.layers.0.weight',\n",
       " 'model.decoder.query_scale.layers.0.bias',\n",
       " 'model.decoder.query_scale.layers.1.weight',\n",
       " 'model.decoder.query_scale.layers.1.bias',\n",
       " 'model.decoder.ref_point_head.layers.0.weight',\n",
       " 'model.decoder.ref_point_head.layers.0.bias',\n",
       " 'model.decoder.ref_point_head.layers.1.weight',\n",
       " 'model.decoder.ref_point_head.layers.1.bias',\n",
       " 'model.decoder.bbox_embed.0.layers.0.weight',\n",
       " 'model.decoder.bbox_embed.0.layers.0.bias',\n",
       " 'model.decoder.bbox_embed.0.layers.1.weight',\n",
       " 'model.decoder.bbox_embed.0.layers.1.bias',\n",
       " 'model.decoder.bbox_embed.0.layers.2.weight',\n",
       " 'model.decoder.bbox_embed.0.layers.2.bias',\n",
       " 'model.decoder.dim_embed.0.layers.0.weight',\n",
       " 'model.decoder.dim_embed.0.layers.0.bias',\n",
       " 'model.decoder.dim_embed.0.layers.1.weight',\n",
       " 'model.decoder.dim_embed.0.layers.1.bias',\n",
       " 'model.target_embeddings.weight',\n",
       " 'model.refpoint_embeddings.weight',\n",
       " 'class_embed.0.weight',\n",
       " 'class_embed.0.bias',\n",
       " 'bbox_embed.0.layers.0.weight',\n",
       " 'bbox_embed.0.layers.0.bias',\n",
       " 'bbox_embed.0.layers.1.weight',\n",
       " 'bbox_embed.0.layers.1.bias',\n",
       " 'bbox_embed.0.layers.2.weight',\n",
       " 'bbox_embed.0.layers.2.bias',\n",
       " 'dim_embed_3d.0.layers.0.weight',\n",
       " 'dim_embed_3d.0.layers.0.bias',\n",
       " 'dim_embed_3d.0.layers.1.weight',\n",
       " 'dim_embed_3d.0.layers.1.bias',\n",
       " 'angle_embed.0.layers.0.weight',\n",
       " 'angle_embed.0.layers.0.bias',\n",
       " 'angle_embed.0.layers.1.weight',\n",
       " 'angle_embed.0.layers.1.bias',\n",
       " 'depth_embed.0.layers.0.weight',\n",
       " 'depth_embed.0.layers.0.bias',\n",
       " 'depth_embed.0.layers.1.weight',\n",
       " 'depth_embed.0.layers.1.bias']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in model_state_dict.keys() if 'backbone' not in k and 'depth_predictor' not in k and 'text_guided_adapter' not in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint = torch.load(\"../../outputs/swin_vl_dab/checkpoint_best.pth\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = checkpoint['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k in state_dict.keys()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
